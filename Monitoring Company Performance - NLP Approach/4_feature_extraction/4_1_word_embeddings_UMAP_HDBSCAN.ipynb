{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc940b4",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495dc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import scipy\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import uuid\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import time\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2e24c",
   "metadata": {},
   "source": [
    "# Defining Functions to Save and Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ede47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from saving_loading_functions.saving_file import saving_file\n",
    "from saving_loading_functions.saving_file_json import saving_file_json\n",
    "from saving_loading_functions.loading_file import loading_file\n",
    "from saving_loading_functions.loading_file_json import loading_file_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7749a1",
   "metadata": {},
   "source": [
    "# Declaring Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring filepaths\n",
    "tokenised_sentences_filepath = 'data/processed/tokenised_sentences/'\n",
    "word_embedding_filepath = 'data/processed/word_embeddings/'\n",
    "umap_filepath = 'data/processed/dimensionality_reduced_word_embeddings/'\n",
    "hdbscan_filepath = 'data/processed/clustered_embeddings/'\n",
    "relative_validity_filepath = 'data/processed/relative_validity/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ea178",
   "metadata": {},
   "source": [
    "# 1) Word Embeddings over Tokenised Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
     ]
    }
   ],
   "source": [
    "# loading embedding model\n",
    "module_url_large = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "model_large = hub.load(module_url_large)\n",
    "print (\"module %s loaded\" % module_url_large)\n",
    "\n",
    "def embed_large(input):\n",
    "    return model_large(input)\n",
    "\n",
    "# declaring filepath to tokenised sentences\n",
    "tokenised_sentences_filepath = 'data/processed/tokenised_sentences/'\n",
    "\n",
    "# declaring bank names\n",
    "bank_names = [*bank names*]\n",
    "\n",
    "for bank in bank_names:\n",
    "    \n",
    "    # loading tokenised sentences\n",
    "    df_poor = loading_file(tokenised_sentences_filepath, f'{bank}_tokenized_sentences_df.csv', 1)\n",
    "    \n",
    "    # getting cleaned sentences\n",
    "    clean_text_list = [sent for sent in df_poor.loc[:, 'cleaned_sentences']]\n",
    "    \n",
    "    # embedding\n",
    "    sentence_embeddings_large = embed_large(clean_text_list)\n",
    "    sentence_embeddings_large_df = pd.DataFrame(sentence_embeddings_large.numpy())\n",
    "    \n",
    "    saving_file(sentence_embeddings_large_df, word_embedding_filepath, f'sentence_embeddings_large_{bank}_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed518b4f",
   "metadata": {},
   "source": [
    "# 2) Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93401f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 15\n",
      "Min Dist = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:28<00:00, 148.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 15\n",
      "Min Dist = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:46<00:00, 106.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 15\n",
      "Min Dist = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:36<00:00, 96.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 50\n",
      "Min Dist = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:36<00:00, 157.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 50\n",
      "Min Dist = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:40<00:00, 100.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 50\n",
      "Min Dist = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:42<00:00, 102.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 100\n",
      "Min Dist = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:01<00:00, 181.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 100\n",
      "Min Dist = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:48<00:00, 108.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "N-neighbors = 100\n",
      "Min Dist = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over:\n",
      "Min Cluster Size = 100\n",
      "Iterating over:\n",
      "Min Cluster Size = 200\n",
      "Iterating over:\n",
      "Min Cluster Size = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:46<00:00, 106.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished in 37656.239334967 seconds\n"
     ]
    }
   ],
   "source": [
    "bank = \"revolut\"\n",
    "\n",
    "# loading sentences and embeddings\n",
    "sentence_embeddings_large_df = loading_file(word_embedding_filepath, f'sentence_embeddings_large_{bank}_df.csv', 1)\n",
    "df_poor = loading_file(tokenised_sentences_filepath, f'{bank}_tokenized_sentences_df.csv', 1)\n",
    "\n",
    "# getting text, sentences and cleaned sentences\n",
    "text_list = [sent for sent in df_poor.loc[:, 'text']]\n",
    "sent_list = [sent for sent in df_poor.loc[:, 'sentences']]\n",
    "clean_text_list = [sent for sent in df_poor.loc[:, 'cleaned_sentences']]\n",
    "\n",
    "# create new df \n",
    "sent_cluster_df = pd.DataFrame({\"text\": text_list, \"sentences\": sent_list, \"cleaned_sentences\": clean_text_list})\n",
    "\n",
    "# dimensionality reduction and clustering\n",
    "def task(n_neighbours_list, min_dist_list, process):\n",
    "\n",
    "    min_cluster_size_list = [100, 200, 300]\n",
    "\n",
    "    min_samples = 1\n",
    "    leaf_size = 20\n",
    "\n",
    "    len_n_neighbours = len(n_neighbours_list)\n",
    "    len_min_dist = len(min_dist_list)\n",
    "    len_min_cluster = len(min_cluster_size_list)\n",
    "\n",
    "    file_names = []\n",
    "    for i in range(len_n_neighbours):\n",
    "        for j in range(len_min_dist):\n",
    "            \n",
    "            # declaring hyperparameter\n",
    "            n_neighbors = n_neighbours_list[i]\n",
    "            min_dist = min_dist_list[j]\n",
    "            col_name = f\"n_neighbors_{n_neighbors}_min_dist_{min_dist}\"\n",
    "            file_name = col_name + f\"_embedding_umap_{bank}.csv\"\n",
    "\n",
    "            print(f\"Iterating over:\\nN-neighbors = {n_neighbors}\\nMin Dist = {min_dist}\")\n",
    "\n",
    "            # dimensionality reduction via UMAP\n",
    "            reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=3, min_dist=min_dist, metric='cosine', \n",
    "                                random_state=42, negative_sample_rate=300)\n",
    "            embedding_umap_3d_large = reducer.fit_transform(sentence_embeddings_large_df)\n",
    "            embedding_umap_3d_large_df = pd.DataFrame(embedding_umap_3d_large)\n",
    "            \n",
    "            # saving reduced embeddings\n",
    "            saving_file(embedding_umap_3d_large_df, umap_filepath, file_name)\n",
    "\n",
    "            file_names.append(file_name)\n",
    "\n",
    "    relative_validity_df = pd.DataFrame({\"Evaluation Metric\": [\"Relative Validity (DBCV)\"]})\n",
    "    for file in tqdm(file_names):\n",
    "\n",
    "        # to account for any memory leaks\n",
    "        from hdbscan import HDBSCAN, prediction\n",
    "\n",
    "        # loading file from bucket\n",
    "        embedding_umap_3d_large_df = loading_file(umap_filepath, file, 1)\n",
    "\n",
    "        # iterate over each min_cluster_size\n",
    "        for k in range(len_min_cluster):\n",
    "            min_cluster_size = min_cluster_size_list[k]\n",
    "\n",
    "            print(f\"Iterating over:\\nMin Cluster Size = {min_cluster_size}\")\n",
    "\n",
    "            # clustering via HDBSCAN\n",
    "            clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, leaf_size=leaf_size,\n",
    "                                prediction_data=True, gen_min_span_tree=True).fit(embedding_umap_3d_large_df)\n",
    "\n",
    "            \n",
    "            col_name = file[:-len(f\"_embedding_umap_{bank}.csv\")] + \"_min_cluster_{}\".format(min_cluster_size)\n",
    "            col_name_prob = col_name + \"_prob\"\n",
    "\n",
    "            # writing cluster onto dataframe\n",
    "            sent_cluster_df[col_name] = clusterer.labels_\n",
    "\n",
    "            # writing cluster probability onto df\n",
    "            cluster_prob = prediction.all_points_membership_vectors(clusterer)\n",
    "            sent_cluster_df[col_name_prob] = 0\n",
    "            for i in range(len(sent_cluster_df)):\n",
    "                entry_cluster = sent_cluster_df.loc[i, col_name]\n",
    "                if entry_cluster != -1:\n",
    "                    sent_cluster_df.loc[i, col_name_prob] = cluster_prob[i][entry_cluster]\n",
    "\n",
    "            # saving sentence-cluster file onto bucket\n",
    "            saving_file(sent_cluster_df, hdbscan_filepath, f'sentence_clustering_{bank}_{process}.csv')\n",
    "            \n",
    "            # writing relative validity onto dataframe        \n",
    "            relative_validity_df[col_name] = clusterer.relative_validity_\n",
    "\n",
    "            # saving relative validity file onto bucket\n",
    "            saving_file(relative_validity_df, relative_validity_filepath, f'relative_validity_{bank}_{process}.csv')\n",
    "\n",
    "# declaring hyperparameters\n",
    "n_neighbours_list = [15, 50, 100]\n",
    "min_dist_list = [0.0, 0.5, 0.99]\n",
    "\n",
    "# track process number\n",
    "i = 0\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    start_time = time.perf_counter()\n",
    "    processes = []\n",
    "\n",
    "    for n_neighbours in n_neighbours_list:\n",
    "        for min_dist in min_dist_list:\n",
    "            i += 1\n",
    "            p = multiprocessing.Process(target = task([n_neighbours], [min_dist], i))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    finish_time = time.perf_counter()\n",
    " \n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364c0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
