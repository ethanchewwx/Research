{"cells":[{"cell_type":"markdown","source":["# **Data Pre-Processing (Reviews)**"],"metadata":{"id":"EdN4n-F4GWxq"},"id":"EdN4n-F4GWxq"},{"cell_type":"markdown","id":"66d62d3f","metadata":{"id":"66d62d3f"},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"id":"f96bf1fb","metadata":{"id":"f96bf1fb"},"outputs":[],"source":["import pandas as pd\n","import spacy\n","import scipy\n","import numpy as np\n","import uuid"]},{"cell_type":"markdown","id":"ad124509","metadata":{"id":"ad124509"},"source":["# Importing Functions"]},{"cell_type":"code","execution_count":null,"id":"0d2d997d","metadata":{"id":"0d2d997d"},"outputs":[],"source":["from data_cleaning_functions.clean_text import clean_text\n","from saving_loading_functions.saving_file import saving_file\n","from saving_loading_functions.loading_file import loading_file"]},{"cell_type":"markdown","id":"5d5440c3","metadata":{"id":"5d5440c3"},"source":["# Declaring Filepaths"]},{"cell_type":"code","execution_count":null,"id":"67970ae3","metadata":{"id":"67970ae3"},"outputs":[],"source":["raw_filepath = 'data/raw/'\n","uuid_filepath = 'data/processed/uuid_dataframes/'\n","adhoc_filepath = 'data/processed/adhoc_fixes/'\n","filtered_filepath = 'data/processed/filtered_dataframes/'\n","tokenised_filepath = 'data/processed/tokenised_sentences/'"]},{"cell_type":"markdown","id":"8bc2fe51","metadata":{"id":"8bc2fe51"},"source":["# Loading Datasets from S3 Bucket"]},{"cell_type":"code","execution_count":null,"id":"07ec60ab","metadata":{"id":"07ec60ab"},"outputs":[],"source":["# loading digital and conventional bank dataset\n","dig_df = loading_file(raw_filepath, 'digital_bank_scraped_data.csv', 0)\n","conven_df = loading_file(raw_filepath, 'conventional_bank_scraped_data.csv', 0)\n","n26_df = loading_file(raw_filepath, 'n26_bank_scraped_data.csv', 0)"]},{"cell_type":"markdown","id":"1ad18c41","metadata":{"id":"1ad18c41"},"source":["# 1) Assigning UUID"]},{"cell_type":"markdown","id":"aad2917d","metadata":{"id":"aad2917d"},"source":["UUID serves as a unique ID for data engineering further down the pipeline"]},{"cell_type":"code","execution_count":null,"id":"d527542a","metadata":{"id":"d527542a"},"outputs":[],"source":["############## assigning UUID ##########################\n","# digital bank\n","dig_df[\"uuid\"] = 0\n","for i in range(len(dig_df)):\n","    dig_df.loc[i, \"uuid\"] = uuid.uuid4()\n","\n","# conventional bank\n","conven_df[\"uuid\"] = 0\n","for i in range(len(conven_df)):\n","    conven_df.loc[i, \"uuid\"] = uuid.uuid4()  \n","\n","# n26\n","n26_df[\"uuid\"] = 0\n","for i in range(len(n26_df)):\n","    n26_df.loc[i, \"uuid\"] = uuid.uuid4()  \n","\n","############ saving files onto bucket ####################\n","# saving resampled dataset\n","saving_file(dig_df, uuid_filepath, 'digital_bank_scraped_data_uuid.csv')\n","saving_file(conven_df, uuid_filepath, 'conventional_bank_scraped_data_uuid.csv')\n","saving_file(n26_df, uuid_filepath, 'n26_bank_scraped_data_uuid.csv')"]},{"cell_type":"markdown","id":"98a3a66c","metadata":{"id":"98a3a66c"},"source":["# 2) Merging Onto Single Dataframe"]},{"cell_type":"code","execution_count":null,"id":"6952e970","metadata":{"id":"6952e970"},"outputs":[],"source":["# merging\n","df = pd.concat([dig_df, conven_df, n26_df], ignore_index=True)\n","\n","# saving dataframe\n","saving_file(df, uuid_filepath, 'all_bank_scraped_data_missing_dates_uuid.csv')"]},{"cell_type":"markdown","id":"0ee10409","metadata":{"id":"0ee10409"},"source":["# 3) Identifying and Rescraping Reviews with Missing Dates"]},{"cell_type":"code","execution_count":null,"id":"76f824bf","metadata":{"id":"76f824bf"},"outputs":[],"source":["# identifying user_urls with missing dates\n","rem_users_df = df.loc[pd.isna(df.loc[:, \"date\"]), \"user_url\"]\n","\n","# saving dataframe\n","saving_file(rem_users_df, adhoc_filepath, 'review_missing_dates_user_urls.csv')"]},{"cell_type":"code","execution_count":null,"id":"005b442c","metadata":{"id":"005b442c","outputId":"1e88dce2-3f19-427b-8d82-04e9caff8284"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_url</th>\n","      <th>company</th>\n","      <th>date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/users/60636a29bd9132001982c2fa</td>\n","      <td>Monzo</td>\n","      <td>2022-06-09T16:57:55.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/users/62a2fabe6d0355001101eb11</td>\n","      <td>Wise (formerly TransferWise)</td>\n","      <td>2022-06-10T10:23:07.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/users/629899df1f28f40011af0c71</td>\n","      <td>Starling Bank</td>\n","      <td>2022-06-09T15:19:53.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/users/5ecfa7dd75750c96c6efb734</td>\n","      <td>Starling Bank</td>\n","      <td>2022-06-09T08:23:40.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/users/56583efd0000ff0001ee1b9c</td>\n","      <td>Starling Bank</td>\n","      <td>2022-07-05T14:04:25.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11377</th>\n","      <td>/users/501c06e400006400011e919f</td>\n","      <td>Wise (formerly TransferWise)</td>\n","      <td>2013-06-27T11:04:37.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>11378</th>\n","      <td>/users/50168b6100006400011e3e15</td>\n","      <td>Wise (formerly TransferWise)</td>\n","      <td>2012-07-30T13:29:25.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>11379</th>\n","      <td>/users/50082d9100006400011d94d5</td>\n","      <td>Wise (formerly TransferWise)</td>\n","      <td>2012-07-19T16:07:01.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>11380</th>\n","      <td>/users/50081f2900006400011d93e9</td>\n","      <td>Wise (formerly TransferWise)</td>\n","      <td>2012-07-19T15:15:41.000Z</td>\n","    </tr>\n","    <tr>\n","      <th>11381</th>\n","      <td>/users/50c1ecc70000640001294fa1</td>\n","      <td>Wise (formerly TransferWise)</td>\n","      <td>2012-12-07T13:22:03.000Z</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11382 rows Ã— 3 columns</p>\n","</div>"],"text/plain":["                              user_url                       company  \\\n","0      /users/60636a29bd9132001982c2fa                         Monzo   \n","1      /users/62a2fabe6d0355001101eb11  Wise (formerly TransferWise)   \n","2      /users/629899df1f28f40011af0c71                 Starling Bank   \n","3      /users/5ecfa7dd75750c96c6efb734                 Starling Bank   \n","4      /users/56583efd0000ff0001ee1b9c                 Starling Bank   \n","...                                ...                           ...   \n","11377  /users/501c06e400006400011e919f  Wise (formerly TransferWise)   \n","11378  /users/50168b6100006400011e3e15  Wise (formerly TransferWise)   \n","11379  /users/50082d9100006400011d94d5  Wise (formerly TransferWise)   \n","11380  /users/50081f2900006400011d93e9  Wise (formerly TransferWise)   \n","11381  /users/50c1ecc70000640001294fa1  Wise (formerly TransferWise)   \n","\n","                           date  \n","0      2022-06-09T16:57:55.000Z  \n","1      2022-06-10T10:23:07.000Z  \n","2      2022-06-09T15:19:53.000Z  \n","3      2022-06-09T08:23:40.000Z  \n","4      2022-07-05T14:04:25.000Z  \n","...                         ...  \n","11377  2013-06-27T11:04:37.000Z  \n","11378  2012-07-30T13:29:25.000Z  \n","11379  2012-07-19T16:07:01.000Z  \n","11380  2012-07-19T15:15:41.000Z  \n","11381  2012-12-07T13:22:03.000Z  \n","\n","[11382 rows x 3 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# after scraping for given users and uploading file onto bucket\n","# loading file from bucket\n","rem_users_data_df = loading_file(adhoc_filepath, 'review_missing_dates_user_urls_data.csv', 0)\n","rem_users_data_df"]},{"cell_type":"code","execution_count":null,"id":"d931a196","metadata":{"id":"d931a196","outputId":"282dd756-2e1c-49d2-f3ad-236f53f15c17"},"outputs":[{"name":"stdout","output_type":"stream","text":["Remaining number of reviews without a date: 24\n"]}],"source":["# replacing the missing dates \n","for i in range(len(rem_users_data_df)):\n","    url = rem_users_data_df.loc[i, \"user_url\"]\n","    company = rem_users_data_df.loc[i, \"company\"]\n","    date = rem_users_data_df.loc[i, \"date\"]\n","    df.loc[(df.loc[:, \"user_url\"] == url) & (df.loc[:, \"company\"] == company), \"date\"] = date\n","\n","print(\"Remaining number of reviews without a date: {}\".format(len(df.loc[pd.isna(df.loc[:, \"date\"]), :])))"]},{"cell_type":"markdown","id":"7d73d02c","metadata":{"id":"7d73d02c"},"source":["Due to the review being removed by the user as of 19/08/2022"]},{"cell_type":"code","execution_count":null,"id":"8270af70","metadata":{"id":"8270af70"},"outputs":[],"source":["# dropping reviews with missing date\n","df.dropna(axis=0, subset=['date'], inplace=True)\n","df.reset_index(drop=True, inplace=True)\n","\n","# saving dataframe\n","saving_file(df, uuid_filepath, 'all_bank_scraped_data_filled_dates_uuid.csv')"]},{"cell_type":"markdown","id":"8350433c","metadata":{"id":"8350433c"},"source":["# 4) Filtering for Poor Reviews, Dropping Duplicates and Replacing Empty Texts with Titles"]},{"cell_type":"code","execution_count":null,"id":"06c153bd","metadata":{"id":"06c153bd","outputId":"a4f38975-ceba-4dad-f869-1385a8360696"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  isetter(loc, value)\n"]}],"source":["def filter_df(bank):\n","\"\"\"\n","Filter a dataset for the given bank, saving the resulting the dataset\n","\"\"\"\n","\n","    # filtering for only reviews of that bank name\n","    filter_df = df.loc[df.loc[:, 'company'] == bank, :]\n","\n","    # filtering for poor reviews\n","    filter_df_poor = filter_df.loc[(filter_df.loc[:, 'rating'] == 'Rated 1 out of 5 stars') | \n","                                   (filter_df.loc[:, 'rating'] == 'Rated 2 out of 5 stars') | \n","                                   (filter_df.loc[:, 'rating'] == 'Rated 3 out of 5 stars'), :]\n","    filter_df_poor.drop_duplicates() # dropping duplicates\n","    filter_df_poor.reset_index(drop=True, inplace=True)\n","\n","    # replacing empty texts with titles (i.e. use title instead if no text)\n","    for i in range(len(filter_df_poor)):\n","        if pd.isnull(filter_df_poor.loc[i, \"text\"]):\n","            filter_df_poor.loc[i, \"text\"] = filter_df_poor.loc[i, \"title\"]\n","    \n","    bank_name = bank.split(\" \")[0].lower()\n","    \n","    # saving dataframe\n","    saving_file(filter_df_poor, filtered_filepath, f'{bank_name}_df_poor.csv')\n","    \n","# iterating over all banks and filtering\n","formal_bank_names = list(set(df.loc[:, \"company\"]))\n","\n","for bank in formal_bank_names:\n","    filter_df(bank)"]},{"cell_type":"markdown","id":"b4cf6b3c","metadata":{"id":"b4cf6b3c"},"source":["# 4) Sentence Tokenization and Additional Cleaning"]},{"cell_type":"markdown","id":"80d975aa","metadata":{"id":"80d975aa"},"source":["After tokenising the text into sentences, the following data cleaning steps are taken:\n"," - Removing NA's \n"," - Removing non-sensical sentences (i.e. sentences with fewer than 3 words) \n"," - Removing duplicated cleaned sentences "]},{"cell_type":"code","execution_count":null,"id":"42fb9e4a","metadata":{"id":"42fb9e4a","outputId":"2f9ee5b3-fa84-4aec-b2ae-41306a1a0417"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","N26\n","Original number of cleaned sentences: 13846\n","Number of cleaned sentences after dropping NA: 13804\n","Number of cleaned sentences after dropping non-sensical sentences: 13199\n","Number of cleaned sentences after dropping duplicates: 13078\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","Token indices sequence length is longer than the specified maximum sequence length for this model (3335 > 512). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["\n","Wise\n","Original number of cleaned sentences: 42354\n","Number of cleaned sentences after dropping NA: 42280\n","Number of cleaned sentences after dropping non-sensical sentences: 40608\n","Number of cleaned sentences after dropping duplicates: 40160\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Starling\n","Original number of cleaned sentences: 21571\n","Number of cleaned sentences after dropping NA: 21524\n","Number of cleaned sentences after dropping non-sensical sentences: 20680\n","Number of cleaned sentences after dropping duplicates: 20516\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Monzo\n","Original number of cleaned sentences: 13044\n","Number of cleaned sentences after dropping NA: 13009\n","Number of cleaned sentences after dropping non-sensical sentences: 12436\n","Number of cleaned sentences after dropping duplicates: 12307\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Revolut\n","Original number of cleaned sentences: 60092\n","Number of cleaned sentences after dropping NA: 60008\n","Number of cleaned sentences after dropping non-sensical sentences: 57730\n","Number of cleaned sentences after dropping duplicates: 57055\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Hsbc\n","Original number of cleaned sentences: 32739\n","Number of cleaned sentences after dropping NA: 32696\n","Number of cleaned sentences after dropping non-sensical sentences: 31551\n","Number of cleaned sentences after dropping duplicates: 31151\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Lloyds\n","Original number of cleaned sentences: 13295\n","Number of cleaned sentences after dropping NA: 13267\n","Number of cleaned sentences after dropping non-sensical sentences: 12742\n","Number of cleaned sentences after dropping duplicates: 12664\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Natwest\n","Original number of cleaned sentences: 19988\n","Number of cleaned sentences after dropping NA: 19949\n","Number of cleaned sentences after dropping non-sensical sentences: 19135\n","Number of cleaned sentences after dropping duplicates: 18947\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Barclays\n","Original number of cleaned sentences: 5986\n","Number of cleaned sentences after dropping NA: 5967\n","Number of cleaned sentences after dropping non-sensical sentences: 5689\n","Number of cleaned sentences after dropping duplicates: 5646\n"]},{"name":"stderr","output_type":"stream","text":["/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/py8_nlp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Santander\n","Original number of cleaned sentences: 23378\n","Number of cleaned sentences after dropping NA: 23340\n","Number of cleaned sentences after dropping non-sensical sentences: 22512\n","Number of cleaned sentences after dropping duplicates: 22175\n"]}],"source":["# load English tokenizer, tagger, parser and NER\n","nlp = spacy.load(\"en_core_web_trf\")\n","\n","def sent_token_clean(bank):\n","    \"\"\"\n","    Takes a dataset of reviews, splits it by bank, tokenises the sentences, \n","    cleans it, and saves the resulting file\n","    \"\"\"\n","\n","    bank_name = bank.split(\" \")[0].lower()\n","    \n","    # loading dataset\n","    df = loading_file(filtered_filepath, f'{bank_name}_df_poor.csv', 1)\n","    \n","    # sentence tokenisation\n","    sent_list = []\n","    text_list = []\n","    for i in range(len(df)):\n","        text = df.loc[i, \"text\"]\n","        if pd.isnull(text):\n","            pass\n","        else:\n","            doc = nlp(text)\n","            entry_sent_list = [str(sentence) for sentence in doc.sents if str(sentence) not in sent_list]\n","            entry_text_list = [str(text) for sentence in doc.sents if str(sentence) not in sent_list]\n","            sent_list.extend(entry_sent_list)\n","            text_list.extend(entry_text_list)\n","    \n","    # cleaning tokenised sentences\n","    cleaned_sent_list = [clean_text(sent, bank_name) for sent in sent_list]\n","    \n","    # creating new dataframe with original review, tokenised sentence and cleaned sentence\n","    sent_dict = {\"text\": text_list, \"sentences\": sent_list, \"cleaned_sentences\": cleaned_sent_list}\n","    bank_sent_df = pd.DataFrame(sent_dict)\n","    \n","    print(\"\\n{}\".format(bank_name.title()))\n","    \n","    # removing NA and duplicated cleaned sentences\n","    print(\"Original number of cleaned sentences: {}\".format(len(bank_sent_df)))\n","\n","    # removing na's\n","    bank_sent_df = bank_sent_df[np.where((bank_sent_df['cleaned_sentences'].str.len()>0), True, False)]\n","    print(\"Number of cleaned sentences after dropping NA: {}\".format(len(bank_sent_df)))\n","\n","    # removing non-sensical sentences (i.e. sentences with length <= 2)\n","    bank_sent_df = bank_sent_df[np.where((bank_sent_df['cleaned_sentences'].str.split(\" \").str.len()>2), True, False)]\n","    print(\"Number of cleaned sentences after dropping non-sensical sentences: {}\".format(len(bank_sent_df)))\n","    \n","    # removing duplicated cleaned sentences\n","    bank_sent_df.drop_duplicates(subset=['cleaned_sentences'], inplace=True)\n","    print(\"Number of cleaned sentences after dropping duplicates: {}\".format(len(bank_sent_df)))\n","\n","    bank_sent_df.reset_index(drop=True, inplace=True)\n","    \n","    # saving file onto bucket\n","    saving_file(bank_sent_df, tokenised_filepath, f'{bank_name}_tokenized_sentences_df.csv')\n","\n","\n","# iterating over different banks\n","for bank in formal_bank_names:\n","    sent_token_clean(bank)"]}],"metadata":{"kernelspec":{"display_name":"Custom (py8_nlp)","language":"python","name":"py8_nlp"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}