{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc0f3b5-d56b-4d3b-8d13-5a8bf4a96bae",
   "metadata": {
    "id": "0fc0f3b5-d56b-4d3b-8d13-5a8bf4a96bae"
   },
   "source": [
    "# DQN on Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUqvJEb_-jP9",
   "metadata": {
    "id": "ZUqvJEb_-jP9"
   },
   "source": [
    "## Downloading ROMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2T2aqkbkqB2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c2T2aqkbkqB2",
    "outputId": "a7d516d8-e81a-43cd-bccd-1d52d749231b"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
    "!pip install unrar\n",
    "!unrar x Roms.rar\n",
    "\n",
    "!pip3 install atari-py\n",
    "!python -m atari_py.import_roms ROMS\n",
    "!pip install \"gym[atari]\" \"gym[accept-rom-license]\" atari_py\n",
    "!pip install -U \"ray[rllib]==1.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2DHN19RL-uY7",
   "metadata": {
    "id": "2DHN19RL-uY7"
   },
   "source": [
    "## Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "OgGheH-qc-L9",
   "metadata": {
    "id": "OgGheH-qc-L9"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_b = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EkzOoz2Z-xm8",
   "metadata": {
    "id": "EkzOoz2Z-xm8"
   },
   "source": [
    "## Setting up Tune, Evaluation Function and Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdbce7b-8cca-42ff-a039-1dec5f183762",
   "metadata": {
    "id": "0cdbce7b-8cca-42ff-a039-1dec5f183762"
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "\n",
    "def evaluation_fn(result):\n",
    "    return result['episode_reward_mean']\n",
    "\n",
    "\n",
    "def objective_fn(config):\n",
    "\n",
    "    trainer = dqn.DQNTrainer(config=config)\n",
    "\n",
    "    for i in range(100):\n",
    "      # Perform one iteration of training the policy with DQN\n",
    "      result = trainer.train()\n",
    "      intermediate_score = evaluation_fn(result)\n",
    "\n",
    "      # Feed the score back back to Tune.\n",
    "      tune.report(iterations=i, mean_reward=intermediate_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f_UeR008-1he",
   "metadata": {
    "id": "f_UeR008-1he"
   },
   "source": [
    "## Config Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d912b1-ddca-4a14-b35e-af4196edf32c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "26d912b1-ddca-4a14-b35e-af4196edf32c",
    "outputId": "42a709f8-b23d-4745-8e62-b8c8b4dc4b5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "# considering both dueling, double DQN and prioritised replay\n",
    "config[\"dueling\"] = tune.grid_search([True, False])\n",
    "config[\"double_q\"] = tune.grid_search([True, False])\n",
    "config[\"prioritized_replay\"] = tune.grid_search([True, False])\n",
    "config[\"env\"] = 'Breakout-v0'\n",
    "config[\"model\"] = { \"fcnet_hiddens\": [64],\n",
    "                    \"fcnet_activation\": 'relu',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rKjDnKcK-5Ky",
   "metadata": {
    "id": "rKjDnKcK-5Ky"
   },
   "source": [
    "## Running Tune on DQNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "BPAoQuUqla4A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704429
    },
    "id": "BPAoQuUqla4A",
    "outputId": "5fff574c-7f0f-4722-9790-1d32665a0c56"
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "        objective_fn, # train using objective function\n",
    "        metric=\"mean_reward\", # metric to optimise\n",
    "        mode=\"max\", # maximise the mean reward\n",
    "        config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3-8VsEDrz41J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "3-8VsEDrz41J",
    "outputId": "4c4d4b73-67cc-4d05-dbdd-c124a8a1579c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-50430ca9-c376-41f1-8527-6b3ab7aed798\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config/prioritized_replay</th>\n",
       "      <th>config/double_q</th>\n",
       "      <th>config/dueling</th>\n",
       "      <th>mean_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.370000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50430ca9-c376-41f1-8527-6b3ab7aed798')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-50430ca9-c376-41f1-8527-6b3ab7aed798 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-50430ca9-c376-41f1-8527-6b3ab7aed798');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   config/prioritized_replay  config/double_q  config/dueling  mean_reward\n",
       "0                       True             True            True     2.666667\n",
       "1                       True            False            True     5.360000\n",
       "2                       True             True           False     5.680000\n",
       "3                       True            False           False     6.620000\n",
       "4                      False             True            True     1.910000\n",
       "5                      False            False            True     3.620000\n",
       "6                      False             True           False     1.830000\n",
       "7                      False            False           False     2.370000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Grid Search Over DQNs\n",
    "df = analysis.dataframe(metric=\"mean_reward\", mode=\"max\")\n",
    "df[['config/prioritized_replay', 'config/double_q', 'config/dueling', 'mean_reward']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3m1ZHAjg_Ail",
   "metadata": {
    "id": "3m1ZHAjg_Ail"
   },
   "source": [
    "## Tuning DQN with PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SOnTdWjWnZx5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786400
    },
    "id": "SOnTdWjWnZx5",
    "outputId": "e58322cc-ed86-4e26-bc23-c92d9cc7c1b6"
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "import gym\n",
    "\n",
    "# ray.init()\n",
    "config_2 = dqn.DEFAULT_CONFIG.copy()\n",
    "# considering dueling, double DQN and prioritised replay\n",
    "config_2[\"dueling\"] = False\n",
    "config_2[\"double_q\"] = False\n",
    "config_2[\"prioritized_replay\"] = True\n",
    "config_2[\"env\"] = 'Breakout-v0'\n",
    "config_2[\"model\"] = { \"fcnet_hiddens\": [64],\n",
    "                    \"fcnet_activation\": 'relu',\n",
    "    }\n",
    "config_2[\"prioritized_replay_alpha\"] = tune.grid_search([0.1, 0.6, 1])\n",
    "config_2[\"prioritized_replay_beta\"] = tune.grid_search([0, 0.4, 1])\n",
    "# config_2[\"num_gpus\"] = 1\n",
    "\n",
    "#@title Running Tune\n",
    "analysis_2 = tune.run(\n",
    "        objective_fn, # train using objective function\n",
    "        metric=\"mean_reward\", # metric to optimise\n",
    "        mode=\"max\", # maximise the mean reward\n",
    "        config=config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nM7ZRqUBy9IW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "nM7ZRqUBy9IW",
    "outputId": "8a24c866-ee0e-41ba-8be7-4fe1d87d661f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1668685b-0047-4ab5-92b3-1555fdf21d8e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config/prioritized_replay_alpha</th>\n",
       "      <th>config/prioritized_replay_beta</th>\n",
       "      <th>mean_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1668685b-0047-4ab5-92b3-1555fdf21d8e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1668685b-0047-4ab5-92b3-1555fdf21d8e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1668685b-0047-4ab5-92b3-1555fdf21d8e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   config/prioritized_replay_alpha  config/prioritized_replay_beta  \\\n",
       "0                              0.1                             0.0   \n",
       "1                              0.6                             0.0   \n",
       "2                              1.0                             0.0   \n",
       "3                              0.1                             0.4   \n",
       "4                              0.6                             0.4   \n",
       "5                              1.0                             0.4   \n",
       "6                              0.1                             1.0   \n",
       "7                              0.6                             1.0   \n",
       "8                              1.0                             1.0   \n",
       "\n",
       "   mean_reward  \n",
       "0     3.420000  \n",
       "1     5.100000  \n",
       "2     4.710000  \n",
       "3     4.310000  \n",
       "4     3.230000  \n",
       "5     4.780000  \n",
       "6     3.990000  \n",
       "7     1.950000  \n",
       "8     2.333333  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = analysis_2.dataframe(metric=\"mean_reward\", mode=\"max\")\n",
    "df_2[['config/prioritized_replay_alpha', 'config/prioritized_replay_beta', 'mean_reward']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mMdc6Hh9_Jz2",
   "metadata": {
    "id": "mMdc6Hh9_Jz2"
   },
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XJkn_-HH-ez_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XJkn_-HH-ez_",
    "outputId": "c911bebe-ae1d-4d4d-9c94-9f00741b26b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2022-04-20 18:26:13,554\tINFO trainable.py:109 -- Trainable.setup took 19.640 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-04-20 18:26:13,558\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-04-20 18:26:31,425\tWARNING deprecation.py:39 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_18-38-03\n",
      "done: false\n",
      "episode_len_mean: 213.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.8\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 10\n",
      "episodes_total: 10\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.0017715163994580507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008319293148815632\n",
      "        model: {}\n",
      "        policy_loss: -0.010277577675879002\n",
      "        total_loss: 1.2135791778564453\n",
      "        vf_explained_var: -0.21532458066940308\n",
      "        vf_loss: 1.2221927642822266\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.6985192497532\n",
      "  ram_util_percent: 25.638598223099706\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.160392375661992\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.202556229781533\n",
      "  mean_inference_ms: 5.669978664613616\n",
      "  mean_raw_obs_processing_ms: 0.5041015916678502\n",
      "time_since_restore: 709.9574327468872\n",
      "time_this_iter_s: 709.9574327468872\n",
      "time_total_s: 709.9574327468872\n",
      "timers:\n",
      "  learn_throughput: 5.78\n",
      "  learn_time_ms: 692055.165\n",
      "  load_throughput: 49932190.476\n",
      "  load_time_ms: 0.08\n",
      "  sample_throughput: 224.062\n",
      "  sample_time_ms: 17852.184\n",
      "  update_time_ms: 14.411\n",
      "timestamp: 1650479883\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "checkpoint saved at /root/ray_results/PPO_Breakout-v0_2022-04-20_18-25-533mtvw_et/checkpoint_000001/checkpoint-1\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_18-49-59\n",
      "done: false\n",
      "episode_len_mean: 213.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.8\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 10\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.870257087561612e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.106652518043319e-12\n",
      "        model: {}\n",
      "        policy_loss: -0.004996407777070999\n",
      "        total_loss: 0.031265005469322205\n",
      "        vf_explained_var: 0.39902183413505554\n",
      "        vf_loss: 0.0362614206969738\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.74711067580802\n",
      "  ram_util_percent: 25.80313418217434\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.160392375661992\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.202556229781533\n",
      "  mean_inference_ms: 5.669978664613616\n",
      "  mean_raw_obs_processing_ms: 0.5041015916678502\n",
      "time_since_restore: 1425.660961151123\n",
      "time_this_iter_s: 715.7035284042358\n",
      "time_total_s: 1425.660961151123\n",
      "timers:\n",
      "  learn_throughput: 5.75\n",
      "  learn_time_ms: 695601.676\n",
      "  load_throughput: 58970882.25\n",
      "  load_time_ms: 0.068\n",
      "  sample_throughput: 232.844\n",
      "  sample_time_ms: 17178.876\n",
      "  update_time_ms: 12.675\n",
      "timestamp: 1650480599\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_19-01-26\n",
      "done: false\n",
      "episode_len_mean: 213.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.8\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 10\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8921942618609364e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.051068163923022e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.00035057964851148427\n",
      "        total_loss: 0.008067585527896881\n",
      "        vf_explained_var: 0.38646918535232544\n",
      "        vf_loss: 0.007716996595263481\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.02941777323797\n",
      "  ram_util_percent: 25.605515832482126\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.160392375661992\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.202556229781533\n",
      "  mean_inference_ms: 5.669978664613616\n",
      "  mean_raw_obs_processing_ms: 0.5041015916678502\n",
      "time_since_restore: 2112.6326344013214\n",
      "time_this_iter_s: 686.9716732501984\n",
      "time_total_s: 2112.6326344013214\n",
      "timers:\n",
      "  learn_throughput: 5.819\n",
      "  learn_time_ms: 687384.993\n",
      "  load_throughput: 66576253.968\n",
      "  load_time_ms: 0.06\n",
      "  sample_throughput: 238.401\n",
      "  sample_time_ms: 16778.467\n",
      "  update_time_ms: 13.079\n",
      "timestamp: 1650481286\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_19-13-05\n",
      "done: false\n",
      "episode_len_mean: 213.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.8\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 10\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.892214800986892e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.41555734304825e-16\n",
      "        model: {}\n",
      "        policy_loss: -0.0021169427782297134\n",
      "        total_loss: 0.0030674452427774668\n",
      "        vf_explained_var: 0.21282000839710236\n",
      "        vf_loss: 0.005184395704418421\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_steps_sampled: 16000\n",
      "  num_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.84307228915661\n",
      "  ram_util_percent: 25.597891566265062\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.160392375661992\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.202556229781533\n",
      "  mean_inference_ms: 5.669978664613616\n",
      "  mean_raw_obs_processing_ms: 0.5041015916678502\n",
      "time_since_restore: 2811.3466510772705\n",
      "time_this_iter_s: 698.7140166759491\n",
      "time_total_s: 2811.3466510772705\n",
      "timers:\n",
      "  learn_throughput: 5.828\n",
      "  learn_time_ms: 686364.915\n",
      "  load_throughput: 66708612.326\n",
      "  load_time_ms: 0.06\n",
      "  sample_throughput: 243.525\n",
      "  sample_time_ms: 16425.397\n",
      "  update_time_ms: 13.173\n",
      "timestamp: 1650481985\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_19-24-36\n",
      "done: false\n",
      "episode_len_mean: 213.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.8\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 10\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8922653161345124e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.437850936210122e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.0027133373077958822\n",
      "        total_loss: 0.026961233466863632\n",
      "        vf_explained_var: -0.15652057528495789\n",
      "        vf_loss: 0.024247901514172554\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.99299492385785\n",
      "  ram_util_percent: 24.767005076142134\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.160392375661992\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.202556229781533\n",
      "  mean_inference_ms: 5.669978664613616\n",
      "  mean_raw_obs_processing_ms: 0.5041015916678502\n",
      "time_since_restore: 3502.5861401557922\n",
      "time_this_iter_s: 691.2394890785217\n",
      "time_total_s: 3502.5861401557922\n",
      "timers:\n",
      "  learn_throughput: 5.847\n",
      "  learn_time_ms: 684090.856\n",
      "  load_throughput: 64182157.613\n",
      "  load_time_ms: 0.062\n",
      "  sample_throughput: 244.181\n",
      "  sample_time_ms: 16381.294\n",
      "  update_time_ms: 13.215\n",
      "timestamp: 1650482676\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_19-36-15\n",
      "done: false\n",
      "episode_len_mean: 1445.9375\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9375\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 16\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.947461441582533e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.050646235636171e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.0050671217031776905\n",
      "        total_loss: 0.051173578947782516\n",
      "        vf_explained_var: -0.20998583734035492\n",
      "        vf_loss: 0.046106453984975815\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_steps_sampled: 24000\n",
      "  num_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.81104417670682\n",
      "  ram_util_percent: 24.785441767068274\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16258773232852752\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.180946555775848\n",
      "  mean_inference_ms: 5.534642671182809\n",
      "  mean_raw_obs_processing_ms: 0.43737921145044945\n",
      "time_since_restore: 4201.9085240364075\n",
      "time_this_iter_s: 699.3223838806152\n",
      "time_total_s: 4201.9085240364075\n",
      "timers:\n",
      "  learn_throughput: 5.849\n",
      "  learn_time_ms: 683918.288\n",
      "  load_throughput: 64075936.346\n",
      "  load_time_ms: 0.062\n",
      "  sample_throughput: 244.565\n",
      "  sample_time_ms: 16355.591\n",
      "  update_time_ms: 13.134\n",
      "timestamp: 1650483375\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "\n",
      "agent_timesteps_total: 28000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_19-47-39\n",
      "done: false\n",
      "episode_len_mean: 1078.5652173913043\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0869565217391304\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 7\n",
      "episodes_total: 23\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0062500000931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6507199213376964e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.761164255582542e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.002823230577632785\n",
      "        total_loss: 0.013470237143337727\n",
      "        vf_explained_var: 0.11719027906656265\n",
      "        vf_loss: 0.010647007264196873\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.02022587268992\n",
      "  ram_util_percent: 24.799383983572902\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1626282698837128\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1649425009501018\n",
      "  mean_inference_ms: 5.443948815811939\n",
      "  mean_raw_obs_processing_ms: 0.4085798855742172\n",
      "time_since_restore: 4885.113163471222\n",
      "time_this_iter_s: 683.2046394348145\n",
      "time_total_s: 4885.113163471222\n",
      "timers:\n",
      "  learn_throughput: 5.868\n",
      "  learn_time_ms: 681657.803\n",
      "  load_throughput: 64210230.727\n",
      "  load_time_ms: 0.062\n",
      "  sample_throughput: 247.332\n",
      "  sample_time_ms: 16172.592\n",
      "  update_time_ms: 12.691\n",
      "timestamp: 1650484059\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_19-59-05\n",
      "done: false\n",
      "episode_len_mean: 1078.5652173913043\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0869565217391304\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 23\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0031250000465661287\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.88256835068168e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.778637486493214e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.014258990064263344\n",
      "        total_loss: 0.019059643149375916\n",
      "        vf_explained_var: -0.15365281701087952\n",
      "        vf_loss: 0.004800656344741583\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_steps_sampled: 32000\n",
      "  num_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.92482124616954\n",
      "  ram_util_percent: 24.793871297242088\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1626282698837128\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1649425009501018\n",
      "  mean_inference_ms: 5.443948815811939\n",
      "  mean_raw_obs_processing_ms: 0.4085798855742172\n",
      "time_since_restore: 5571.766065835953\n",
      "time_this_iter_s: 686.6529023647308\n",
      "time_total_s: 5571.766065835953\n",
      "timers:\n",
      "  learn_throughput: 5.879\n",
      "  learn_time_ms: 680386.339\n",
      "  load_throughput: 66149693.445\n",
      "  load_time_ms: 0.06\n",
      "  sample_throughput: 249.336\n",
      "  sample_time_ms: 16042.635\n",
      "  update_time_ms: 12.322\n",
      "timestamp: 1650484745\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_20-10-33\n",
      "done: false\n",
      "episode_len_mean: 1078.5652173913043\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0869565217391304\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 23\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0015625000232830644\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.88256835068168e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.697165093065967e-16\n",
      "        model: {}\n",
      "        policy_loss: -0.014150742441415787\n",
      "        total_loss: -0.00705003272742033\n",
      "        vf_explained_var: -0.08212103694677353\n",
      "        vf_loss: 0.007100711110979319\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.81948979591834\n",
      "  ram_util_percent: 25.06040816326531\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1626282698837128\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1649425009501018\n",
      "  mean_inference_ms: 5.443948815811939\n",
      "  mean_raw_obs_processing_ms: 0.4085798855742172\n",
      "time_since_restore: 6259.442705154419\n",
      "time_this_iter_s: 687.6766393184662\n",
      "time_total_s: 6259.442705154419\n",
      "timers:\n",
      "  learn_throughput: 5.886\n",
      "  learn_time_ms: 679607.759\n",
      "  load_throughput: 67832409.704\n",
      "  load_time_ms: 0.059\n",
      "  sample_throughput: 252.443\n",
      "  sample_time_ms: 15845.135\n",
      "  update_time_ms: 12.042\n",
      "timestamp: 1650485433\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_20-21-55\n",
      "done: false\n",
      "episode_len_mean: 1078.5652173913043\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0869565217391304\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 23\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0007812500116415322\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8829102993732647e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.962910128560616e-15\n",
      "        model: {}\n",
      "        policy_loss: 0.005655410699546337\n",
      "        total_loss: 0.007198031060397625\n",
      "        vf_explained_var: -0.2407475858926773\n",
      "        vf_loss: 0.0015426198951900005\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.98427543679341\n",
      "  ram_util_percent: 25.1\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1626282698837128\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1649425009501018\n",
      "  mean_inference_ms: 5.443948815811939\n",
      "  mean_raw_obs_processing_ms: 0.4085798855742172\n",
      "time_since_restore: 6941.754383325577\n",
      "time_this_iter_s: 682.3116781711578\n",
      "time_total_s: 6941.754383325577\n",
      "timers:\n",
      "  learn_throughput: 5.897\n",
      "  learn_time_ms: 678280.488\n",
      "  load_throughput: 67814130.962\n",
      "  load_time_ms: 0.059\n",
      "  sample_throughput: 252.285\n",
      "  sample_time_ms: 15855.069\n",
      "  update_time_ms: 11.745\n",
      "timestamp: 1650486115\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "\n",
      "agent_timesteps_total: 44000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_20-33-22\n",
      "done: false\n",
      "episode_len_mean: 1355.7307692307693\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0769230769230769\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 26\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0003906250058207661\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.03675065596687e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -3.040576700704225e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.0052576856687664986\n",
      "        total_loss: 0.020477300509810448\n",
      "        vf_explained_var: 0.01130673848092556\n",
      "        vf_loss: 0.0152196129783988\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.98609406952963\n",
      "  ram_util_percent: 24.948159509202455\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16239772795977012\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.155070658315228\n",
      "  mean_inference_ms: 5.409489937103223\n",
      "  mean_raw_obs_processing_ms: 0.39633691797565906\n",
      "time_since_restore: 7628.193556547165\n",
      "time_this_iter_s: 686.4391732215881\n",
      "time_total_s: 7628.193556547165\n",
      "timers:\n",
      "  learn_throughput: 5.915\n",
      "  learn_time_ms: 676219.451\n",
      "  load_throughput: 72315586.207\n",
      "  load_time_ms: 0.055\n",
      "  sample_throughput: 256.969\n",
      "  sample_time_ms: 15566.078\n",
      "  update_time_ms: 11.255\n",
      "timestamp: 1650486802\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "\n",
      "checkpoint saved at /root/ray_results/PPO_Breakout-v0_2022-04-20_18-25-533mtvw_et/checkpoint_000011/checkpoint-11\n",
      "agent_timesteps_total: 48000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_20-44-47\n",
      "done: false\n",
      "episode_len_mean: 1531.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0333333333333334\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 30\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.00019531250291038305\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.5060594144885613e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.6207866426494015e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.002833342645317316\n",
      "        total_loss: 0.0048594302497804165\n",
      "        vf_explained_var: -0.025593973696231842\n",
      "        vf_loss: 0.0020260869059711695\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_steps_sampled: 48000\n",
      "  num_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.96898669396109\n",
      "  ram_util_percent: 24.90081883316274\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16188362262333417\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.14683875136213\n",
      "  mean_inference_ms: 5.369196416721015\n",
      "  mean_raw_obs_processing_ms: 0.3854050221901615\n",
      "time_since_restore: 8313.644598007202\n",
      "time_this_iter_s: 685.4510414600372\n",
      "time_total_s: 8313.644598007202\n",
      "timers:\n",
      "  learn_throughput: 5.941\n",
      "  learn_time_ms: 673261.914\n",
      "  load_throughput: 73519789.658\n",
      "  load_time_ms: 0.054\n",
      "  sample_throughput: 258.065\n",
      "  sample_time_ms: 15499.945\n",
      "  update_time_ms: 11.148\n",
      "timestamp: 1650487487\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "\n",
      "agent_timesteps_total: 52000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_20-56-12\n",
      "done: false\n",
      "episode_len_mean: 1285.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0810810810810811\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 7\n",
      "episodes_total: 37\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.765625145519152e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4949515503868156e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.316890974885129e-16\n",
      "        model: {}\n",
      "        policy_loss: -0.0008481022086925805\n",
      "        total_loss: 0.004963058512657881\n",
      "        vf_explained_var: 0.24671663343906403\n",
      "        vf_loss: 0.005811164155602455\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.99507692307691\n",
      "  ram_util_percent: 24.88676923076923\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16135945429825946\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1366344867878535\n",
      "  mean_inference_ms: 5.318245244021785\n",
      "  mean_raw_obs_processing_ms: 0.37416309936496334\n",
      "time_since_restore: 8998.315056085587\n",
      "time_this_iter_s: 684.6704580783844\n",
      "time_total_s: 8998.315056085587\n",
      "timers:\n",
      "  learn_throughput: 5.943\n",
      "  learn_time_ms: 673045.532\n",
      "  load_throughput: 70256348.409\n",
      "  load_time_ms: 0.057\n",
      "  sample_throughput: 258.277\n",
      "  sample_time_ms: 15487.256\n",
      "  update_time_ms: 10.697\n",
      "timestamp: 1650488172\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "\n",
      "agent_timesteps_total: 56000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_21-07-42\n",
      "done: false\n",
      "episode_len_mean: 1285.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0810810810810811\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 37\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.882812572759576e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7796017437454736e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -9.793567519117224e-16\n",
      "        model: {}\n",
      "        policy_loss: 0.014091298915445805\n",
      "        total_loss: 0.014149297028779984\n",
      "        vf_explained_var: -0.046225327998399734\n",
      "        vf_loss: 5.79947663936764e-05\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_steps_sampled: 56000\n",
      "  num_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.9670396744659\n",
      "  ram_util_percent: 24.892166836215665\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16135945429825946\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1366344867878535\n",
      "  mean_inference_ms: 5.318245244021785\n",
      "  mean_raw_obs_processing_ms: 0.37416309936496334\n",
      "time_since_restore: 9688.01639008522\n",
      "time_this_iter_s: 689.7013339996338\n",
      "time_total_s: 9688.01639008522\n",
      "timers:\n",
      "  learn_throughput: 5.951\n",
      "  learn_time_ms: 672109.218\n",
      "  load_throughput: 71851032.12\n",
      "  load_time_ms: 0.056\n",
      "  sample_throughput: 257.689\n",
      "  sample_time_ms: 15522.614\n",
      "  update_time_ms: 10.723\n",
      "timestamp: 1650488862\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "\n",
      "agent_timesteps_total: 60000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_21-19-12\n",
      "done: false\n",
      "episode_len_mean: 1285.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0810810810810811\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 37\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.441406286379788e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.78124695549009e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -6.267123084868157e-15\n",
      "        model: {}\n",
      "        policy_loss: -0.000850552401971072\n",
      "        total_loss: -0.0003902600146830082\n",
      "        vf_explained_var: -0.03673302382230759\n",
      "        vf_loss: 0.0004602937842719257\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.95167512690355\n",
      "  ram_util_percent: 24.994619289340108\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16135945429825946\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1366344867878535\n",
      "  mean_inference_ms: 5.318245244021785\n",
      "  mean_raw_obs_processing_ms: 0.37416309936496334\n",
      "time_since_restore: 10378.719188928604\n",
      "time_this_iter_s: 690.7027988433838\n",
      "time_total_s: 10378.719188928604\n",
      "timers:\n",
      "  learn_throughput: 5.952\n",
      "  learn_time_ms: 672048.98\n",
      "  load_throughput: 71120033.913\n",
      "  load_time_ms: 0.056\n",
      "  sample_throughput: 257.585\n",
      "  sample_time_ms: 15528.857\n",
      "  update_time_ms: 10.702\n",
      "timestamp: 1650489552\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "\n",
      "agent_timesteps_total: 64000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_21-30-37\n",
      "done: false\n",
      "episode_len_mean: 1480.7435897435898\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0512820512820513\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 39\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.220703143189894e-05\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1339953415466795e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.610991550019859e-15\n",
      "        model: {}\n",
      "        policy_loss: -0.004548599012196064\n",
      "        total_loss: -0.003385750111192465\n",
      "        vf_explained_var: -0.15976987779140472\n",
      "        vf_loss: 0.001162844244390726\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_steps_sampled: 64000\n",
      "  num_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.97874743326487\n",
      "  ram_util_percent: 25.16006160164271\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16137450241982237\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1339227458541883\n",
      "  mean_inference_ms: 5.309402602789464\n",
      "  mean_raw_obs_processing_ms: 0.370005125051601\n",
      "time_since_restore: 11062.941545724869\n",
      "time_this_iter_s: 684.2223567962646\n",
      "time_total_s: 11062.941545724869\n",
      "timers:\n",
      "  learn_throughput: 5.964\n",
      "  learn_time_ms: 670655.401\n",
      "  load_throughput: 68871986.864\n",
      "  load_time_ms: 0.058\n",
      "  sample_throughput: 259.551\n",
      "  sample_time_ms: 15411.236\n",
      "  update_time_ms: 10.831\n",
      "timestamp: 1650490237\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "\n",
      "agent_timesteps_total: 68000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_21-42-05\n",
      "done: false\n",
      "episode_len_mean: 1480.7435897435898\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.0512820512820513\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 39\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.10351571594947e-06\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9321806094273484e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.5513098239420516e-15\n",
      "        model: {}\n",
      "        policy_loss: -0.014001909643411636\n",
      "        total_loss: -0.013923220336437225\n",
      "        vf_explained_var: -0.07897437363862991\n",
      "        vf_loss: 7.868587999837473e-05\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.88827726809377\n",
      "  ram_util_percent: 24.864627930682975\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16137450241982237\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1339227458541883\n",
      "  mean_inference_ms: 5.309402602789464\n",
      "  mean_raw_obs_processing_ms: 0.370005125051601\n",
      "time_since_restore: 11751.062460899353\n",
      "time_this_iter_s: 688.1209151744843\n",
      "time_total_s: 11751.062460899353\n",
      "timers:\n",
      "  learn_throughput: 5.961\n",
      "  learn_time_ms: 671040.865\n",
      "  load_throughput: 66629134.234\n",
      "  load_time_ms: 0.06\n",
      "  sample_throughput: 257.773\n",
      "  sample_time_ms: 15517.511\n",
      "  update_time_ms: 10.787\n",
      "timestamp: 1650490925\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "\n",
      "agent_timesteps_total: 72000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_21-53-26\n",
      "done: false\n",
      "episode_len_mean: 1693.725\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.05\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 40\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.051757857974735e-06\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5336925052753543e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.0447565929131942e-15\n",
      "        model: {}\n",
      "        policy_loss: -0.0017234472325071692\n",
      "        total_loss: -0.0013097719056531787\n",
      "        vf_explained_var: -0.2551237940788269\n",
      "        vf_loss: 0.0004136736097279936\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_steps_sampled: 72000\n",
      "  num_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.00834191555096\n",
      "  ram_util_percent: 24.887641606591142\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1613233809675582\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.132805293975473\n",
      "  mean_inference_ms: 5.304154799178424\n",
      "  mean_raw_obs_processing_ms: 0.3684712126473787\n",
      "time_since_restore: 12432.271835327148\n",
      "time_this_iter_s: 681.2093744277954\n",
      "time_total_s: 12432.271835327148\n",
      "timers:\n",
      "  learn_throughput: 5.966\n",
      "  learn_time_ms: 670509.232\n",
      "  load_throughput: 66841498.008\n",
      "  load_time_ms: 0.06\n",
      "  sample_throughput: 257.993\n",
      "  sample_time_ms: 15504.287\n",
      "  update_time_ms: 11.262\n",
      "timestamp: 1650491606\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "\n",
      "agent_timesteps_total: 76000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_22-04-53\n",
      "done: false\n",
      "episode_len_mean: 1693.725\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.05\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 40\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5258789289873675e-06\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.404047126982391e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.4845945940076223e-14\n",
      "        model: {}\n",
      "        policy_loss: 0.014012773521244526\n",
      "        total_loss: 0.014014211483299732\n",
      "        vf_explained_var: -0.08281011879444122\n",
      "        vf_loss: 1.4386632756213658e-06\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.92300613496931\n",
      "  ram_util_percent: 24.882822085889572\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1613233809675582\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.132805293975473\n",
      "  mean_inference_ms: 5.304154799178424\n",
      "  mean_raw_obs_processing_ms: 0.3684712126473787\n",
      "time_since_restore: 13119.236288309097\n",
      "time_this_iter_s: 686.9644529819489\n",
      "time_total_s: 13119.236288309097\n",
      "timers:\n",
      "  learn_throughput: 5.966\n",
      "  learn_time_ms: 670441.974\n",
      "  load_throughput: 65844646.782\n",
      "  load_time_ms: 0.061\n",
      "  sample_throughput: 258.061\n",
      "  sample_time_ms: 15500.217\n",
      "  update_time_ms: 11.3\n",
      "timestamp: 1650492293\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "\n",
      "agent_timesteps_total: 80000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_22-16-19\n",
      "done: false\n",
      "episode_len_mean: 1693.725\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 1.05\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 40\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.629394644936838e-07\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.5026074535492455e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -3.5443628826174745e-13\n",
      "        model: {}\n",
      "        policy_loss: -0.0005046179285272956\n",
      "        total_loss: -0.0005038570379838347\n",
      "        vf_explained_var: 0.10359260439872742\n",
      "        vf_loss: 7.511798116865975e-07\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_steps_sampled: 80000\n",
      "  num_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.00040941658136\n",
      "  ram_util_percent: 24.88413510747185\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1613233809675582\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.132805293975473\n",
      "  mean_inference_ms: 5.304154799178424\n",
      "  mean_raw_obs_processing_ms: 0.3684712126473787\n",
      "time_since_restore: 13804.931967496872\n",
      "time_this_iter_s: 685.6956791877747\n",
      "time_total_s: 13804.931967496872\n",
      "timers:\n",
      "  learn_throughput: 5.963\n",
      "  learn_time_ms: 670811.959\n",
      "  load_throughput: 63986331.045\n",
      "  load_time_ms: 0.063\n",
      "  sample_throughput: 258.607\n",
      "  sample_time_ms: 15467.478\n",
      "  update_time_ms: 11.712\n",
      "timestamp: 1650492979\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "\n",
      "agent_timesteps_total: 84000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_22-27-53\n",
      "done: false\n",
      "episode_len_mean: 1744.7111111111112\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9555555555555556\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 45\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.814697322468419e-07\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.142510306477277e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.310561902057028e-13\n",
      "        model: {}\n",
      "        policy_loss: -0.0004864622314926237\n",
      "        total_loss: 0.000587768096011132\n",
      "        vf_explained_var: -0.2744907736778259\n",
      "        vf_loss: 0.0010742308804765344\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.91852226720647\n",
      "  ram_util_percent: 24.879858299595142\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16134479566423557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1260036272788327\n",
      "  mean_inference_ms: 5.283300721513301\n",
      "  mean_raw_obs_processing_ms: 0.35972870152166386\n",
      "time_since_restore: 14498.62426519394\n",
      "time_this_iter_s: 693.6922976970673\n",
      "time_total_s: 14498.62426519394\n",
      "timers:\n",
      "  learn_throughput: 5.957\n",
      "  learn_time_ms: 671481.146\n",
      "  load_throughput: 62438466.691\n",
      "  load_time_ms: 0.064\n",
      "  sample_throughput: 257.676\n",
      "  sample_time_ms: 15523.368\n",
      "  update_time_ms: 11.704\n",
      "timestamp: 1650493673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "\n",
      "checkpoint saved at /root/ray_results/PPO_Breakout-v0_2022-04-20_18-25-533mtvw_et/checkpoint_000021/checkpoint-21\n",
      "agent_timesteps_total: 88000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_22-39-17\n",
      "done: false\n",
      "episode_len_mean: 1744.7111111111112\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9555555555555556\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 45\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9073486612342094e-07\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.115186052506715e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.7690032381653675e-13\n",
      "        model: {}\n",
      "        policy_loss: -0.0009683412499725819\n",
      "        total_loss: -0.0009564656065776944\n",
      "        vf_explained_var: -0.1883440613746643\n",
      "        vf_loss: 1.1874611118400935e-05\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_steps_sampled: 88000\n",
      "  num_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.0459487179487\n",
      "  ram_util_percent: 24.882564102564103\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16134479566423557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1260036272788327\n",
      "  mean_inference_ms: 5.283300721513301\n",
      "  mean_raw_obs_processing_ms: 0.35972870152166386\n",
      "time_since_restore: 15182.8229367733\n",
      "time_this_iter_s: 684.198671579361\n",
      "time_total_s: 15182.8229367733\n",
      "timers:\n",
      "  learn_throughput: 5.958\n",
      "  learn_time_ms: 671381.571\n",
      "  load_throughput: 61862890.855\n",
      "  load_time_ms: 0.065\n",
      "  sample_throughput: 258.109\n",
      "  sample_time_ms: 15497.302\n",
      "  update_time_ms: 11.833\n",
      "timestamp: 1650494357\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "\n",
      "agent_timesteps_total: 92000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_22-50-44\n",
      "done: false\n",
      "episode_len_mean: 1744.7111111111112\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9555555555555556\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 45\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.536743306171047e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.066095321026864e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -3.468313960683367e-13\n",
      "        model: {}\n",
      "        policy_loss: -0.0052063786424696445\n",
      "        total_loss: -0.005188392475247383\n",
      "        vf_explained_var: -0.19001595675945282\n",
      "        vf_loss: 1.798696757759899e-05\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.92257405515831\n",
      "  ram_util_percent: 24.879366700715014\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16134479566423557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1260036272788327\n",
      "  mean_inference_ms: 5.283300721513301\n",
      "  mean_raw_obs_processing_ms: 0.35972870152166386\n",
      "time_since_restore: 15869.380313396454\n",
      "time_this_iter_s: 686.5573766231537\n",
      "time_total_s: 15869.380313396454\n",
      "timers:\n",
      "  learn_throughput: 5.956\n",
      "  learn_time_ms: 671574.314\n",
      "  load_throughput: 62461712.584\n",
      "  load_time_ms: 0.064\n",
      "  sample_throughput: 258.197\n",
      "  sample_time_ms: 15492.027\n",
      "  update_time_ms: 12.222\n",
      "timestamp: 1650495044\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "\n",
      "agent_timesteps_total: 96000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_23-02-04\n",
      "done: false\n",
      "episode_len_mean: 1744.7111111111112\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9555555555555556\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 45\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.7683716530855236e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.272958181201659e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.422110735317203e-13\n",
      "        model: {}\n",
      "        policy_loss: -0.013814222998917103\n",
      "        total_loss: -0.013797301799058914\n",
      "        vf_explained_var: -0.11896150559186935\n",
      "        vf_loss: 1.691596844466403e-05\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_steps_sampled: 96000\n",
      "  num_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.03429454170956\n",
      "  ram_util_percent: 24.899999999999995\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16134479566423557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1260036272788327\n",
      "  mean_inference_ms: 5.283300721513301\n",
      "  mean_raw_obs_processing_ms: 0.35972870152166386\n",
      "time_since_restore: 16550.195746183395\n",
      "time_this_iter_s: 680.8154327869415\n",
      "time_total_s: 16550.195746183395\n",
      "timers:\n",
      "  learn_throughput: 5.964\n",
      "  learn_time_ms: 670689.858\n",
      "  load_throughput: 61931399.04\n",
      "  load_time_ms: 0.065\n",
      "  sample_throughput: 258.254\n",
      "  sample_time_ms: 15488.649\n",
      "  update_time_ms: 11.904\n",
      "timestamp: 1650495724\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "\n",
      "agent_timesteps_total: 100000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_23-13-39\n",
      "done: false\n",
      "episode_len_mean: 1744.7111111111112\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9555555555555556\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 45\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.3841858265427618e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.157910765163365e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0965110113295151e-12\n",
      "        model: {}\n",
      "        policy_loss: 0.013680773787200451\n",
      "        total_loss: 0.013695444911718369\n",
      "        vf_explained_var: -0.09015978127717972\n",
      "        vf_loss: 1.4674453268526122e-05\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.89747474747473\n",
      "  ram_util_percent: 24.899999999999995\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16134479566423557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1260036272788327\n",
      "  mean_inference_ms: 5.283300721513301\n",
      "  mean_raw_obs_processing_ms: 0.35972870152166386\n",
      "time_since_restore: 17245.178100585938\n",
      "time_this_iter_s: 694.9823544025421\n",
      "time_total_s: 17245.178100585938\n",
      "timers:\n",
      "  learn_throughput: 5.96\n",
      "  learn_time_ms: 671155.706\n",
      "  load_throughput: 65179549.34\n",
      "  load_time_ms: 0.061\n",
      "  sample_throughput: 258.889\n",
      "  sample_time_ms: 15450.664\n",
      "  update_time_ms: 11.966\n",
      "timestamp: 1650496419\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "\n",
      "agent_timesteps_total: 104000\n",
      "custom_metrics: {}\n",
      "date: 2022-04-20_23-25-02\n",
      "done: false\n",
      "episode_len_mean: 1924.1739130434783\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0\n",
      "episode_reward_mean: 0.9347826086956522\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 46\n",
      "experiment_id: abad9f50298f464fb0f55c0d705f1b89\n",
      "hostname: d661aa9c8280\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1920929132713809e-08\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.034841432660642e-10\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -9.736895534642742e-13\n",
      "        model: {}\n",
      "        policy_loss: -0.0015991708496585488\n",
      "        total_loss: -0.0012295879423618317\n",
      "        vf_explained_var: -0.05601822957396507\n",
      "        vf_loss: 0.000369583023712039\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_steps_sampled: 104000\n",
      "  num_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 172.28.0.2\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 99.03552361396302\n",
      "  ram_util_percent: 24.900718685831617\n",
      "pid: 2262\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.16136225699110074\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.124863871135282\n",
      "  mean_inference_ms: 5.279742343218523\n",
      "  mean_raw_obs_processing_ms: 0.3581359803755884\n",
      "time_since_restore: 17928.18836593628\n",
      "time_this_iter_s: 683.0102653503418\n",
      "time_total_s: 17928.18836593628\n",
      "timers:\n",
      "  learn_throughput: 5.961\n",
      "  learn_time_ms: 671021.132\n",
      "  load_throughput: 67541127.214\n",
      "  load_time_ms: 0.059\n",
      "  sample_throughput: 258.664\n",
      "  sample_time_ms: 15464.072\n",
      "  update_time_ms: 11.95\n",
      "timestamp: 1650497102\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "import ray\n",
    "import gym\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# ray.init()\n",
    "config_ppo = ppo.DEFAULT_CONFIG.copy()\n",
    "config_ppo[\"env\"] = 'Breakout-v0'\n",
    "trainer = ppo.PPOTrainer(config=config_ppo)\n",
    "\n",
    "for i in range(100):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "   result = trainer.train()\n",
    "\n",
    "# def objective_fn_ppo(config):\n",
    "\n",
    "#     trainer = ppo.PPOTrainer(config=config)\n",
    "\n",
    "#     for i in range(100):\n",
    "#       # Perform one iteration of training the policy with DQN\n",
    "#       result = trainer.train()\n",
    "#       intermediate_score = evaluation_fn(result)\n",
    "\n",
    "#       # Feed the score back back to Tune.\n",
    "#       tune.report(iterations=i, mean_reward=intermediate_score)\n",
    "\n",
    "\n",
    "# analysis_ppo = tune.run(\n",
    "#         objective_fn_ppo, # train using objective function\n",
    "#         metric=\"mean_reward\", # metric to optimise\n",
    "#         mode=\"max\", # maximise the mean reward\n",
    "#         config=config_ppo)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Latest - collab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
